# Jio Internship â€“ Customer Churn Prediction

This repository contains my work during my internship at **Jio India**, focusing on **predicting customer churn** using advanced machine learning techniques.
The project includes **exploratory data analysis**, **feature engineering**, **model training**, and **evaluation**â€”all implemented in Jupyter notebooks.

## ğŸ“Œ Project Overview

Telecom companies face significant revenue losses due to customer churn. This project aims to:

* Analyze customer data
* Identify key factors influencing churn
* Build predictive models to identify at-risk customers
* Provide actionable insights to improve customer retention

## ğŸ—‚ Repository Structure

```
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_Data_Exploration.ipynb
â”‚   â”œâ”€â”€ 02_Feature_Engineering.ipynb
â”‚   â”œâ”€â”€ 03_Model_Training_Evaluation.ipynb
â”‚   â””â”€â”€ 04_Insights_Visualization.ipynb
â”œâ”€â”€ data/               # (Not included â€“ confidential Jio data)
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

## âš™ï¸ Tech Stack

* **Language:** Python
* **Libraries:** pandas, numpy, matplotlib, seaborn, scikit-learn, XGBoost
* **Tools:** Jupyter Notebook

## ğŸ” Key Highlights

* **EDA:** Distribution analysis, correlation heatmaps, missing value treatment
* **Feature Engineering:** Encoding categorical variables, feature scaling
* **Modeling:** Logistic Regression, Random Forest, XGBoost
* **Evaluation:** Accuracy, Precision, Recall, F1-score, ROC-AUC
* **Insights:** Actionable business recommendations for churn reduction

## ğŸš€ How to Run

1. Clone the repository:

   ```
   git clone https://github.com/<username>/jio-internship-notebooks.git
   ```
2. Navigate into the folder:

   ```
   cd jio-internship-notebooks
   ```
3. Install dependencies:

   ```
   pip install -r requirements.txt
   ```
4. Open Jupyter Notebook:

   ```
   jupyter notebook
   ```
5. Run notebooks in sequence.

## ğŸ“„ Disclaimer

The original dataset from Jio is **confidential** and **not shared** in this repository.
Notebooks are structured to work with similar datasets.

---

