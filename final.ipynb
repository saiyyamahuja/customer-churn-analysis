{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faef3a10-c410-4342-8f67-44552c679b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-03 15:26:37.700934: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b125cce1-4def-4f1e-9a5d-204283580fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"WA_Fn-UseC_-Telco-Customer-Churn.csv\")[:1000]\n",
    "\n",
    "# Preprocessing\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "df['TotalCharges'] = df['TotalCharges'].fillna(df['TotalCharges'].median())\n",
    "\n",
    "# Define categorical columns and encode them using OneHotEncoder\n",
    "categorical_columns = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', \n",
    "                       'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', \n",
    "                       'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', \n",
    "                       'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
    "\n",
    "df['Churn'] = df['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "992569cc-49ca-40e8-8474-c06f017f173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding\n",
    "one_hot_enc = OneHotEncoder()\n",
    "transformer = ColumnTransformer([('one_hot_enc', one_hot_enc, categorical_columns)], remainder='passthrough')\n",
    "df_transformed = transformer.fit_transform(df.drop(columns=['customerID', 'Churn']))\n",
    "\n",
    "# Split data into features and target\n",
    "X = pd.DataFrame(df_transformed)\n",
    "y = df['Churn']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizing features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e1bd2ab-e082-4c7e-affe-d0e41ac7d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and their hyperparameters\n",
    "models = {\n",
    "    'LogisticRegression': {\n",
    "        'model': LogisticRegression(),\n",
    "        'params': {\n",
    "            'C': stats.loguniform(0.001, 1000),\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['liblinear', 'saga'],\n",
    "            'max_iter': [10000],\n",
    "            'class_weight': [None, 'balanced']\n",
    "        }\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200, 300],\n",
    "            'max_depth': [5, 10, 20, None],\n",
    "            'class_weight': [None, 'balanced']\n",
    "        }\n",
    "    },\n",
    "    'SVC': {\n",
    "        'model': SVC(),\n",
    "        'params': {\n",
    "            'C': stats.loguniform(0.1, 100),\n",
    "            'kernel': ['linear', 'rbf'],\n",
    "            'class_weight': [None, 'balanced']\n",
    "        }\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'max_depth': [5, 10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'class_weight': [None, 'balanced']\n",
    "        }\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {\n",
    "            'n_neighbors': [3, 5, 7, 9, 11, 13],\n",
    "            'weights': ['uniform', 'distance']\n",
    "        }\n",
    "    },\n",
    "    'GaussianNB': {\n",
    "        'model': GaussianNB(),\n",
    "        'params': {\n",
    "            'var_smoothing': stats.loguniform(1e-9, 1e-5)\n",
    "        }\n",
    "    },\n",
    "    'XGBClassifier': {\n",
    "        'model': XGBClassifier(),\n",
    "        'params': {\n",
    "            'learning_rate': stats.loguniform(0.01, 0.2),\n",
    "            'max_depth': [3, 5, 7, 10],\n",
    "            'n_estimators': [100, 200, 300]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfafb998-0f33-4021-8d2b-948f813160f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to perform RandomizedSearchCV\n",
    "def perform_random_search(model, param_dist, X_train, y_train, n_iter=100):\n",
    "    clf = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=n_iter, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf.best_estimator_, clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12437a80-46d7-4e4b-bdfe-018e0c3b67f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LogisticRegression: {'C': 4.418441521199722, 'class_weight': None, 'max_iter': 10000, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "\n",
      "Best parameters for RandomForestClassifier: {'n_estimators': 300, 'max_depth': 10, 'class_weight': None}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform hyperparameter tuning\n",
    "best_models = {}\n",
    "for model_name, model_details in models.items():\n",
    "    model = model_details['model']\n",
    "    param_dist = model_details['params']\n",
    "    best_model, best_params = perform_random_search(model, param_dist, X_train, y_train)\n",
    "    best_models[model_name] = best_model\n",
    "    print(f\"Best parameters for {model_name}: {best_params}\\n\")\n",
    "\n",
    "# Evaluate models\n",
    "for model_name, model in best_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"\\n{model_name} Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "\n",
    "# Neural Network\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c1318-a967-4111-836a-015c2953a114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=2, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_nn = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "print(\"Neural Network Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_nn))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e26a2e3-0281-44ce-b454-0008362d7d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "# Plotting the confusion matrix for the best model\n",
    "best_model_name = max(best_models, key=lambda name: accuracy_score(y_test, best_models[name].predict(X_test)))\n",
    "best_model = best_models[best_model_name]\n",
    "conf_matrix = confusion_matrix(y_test, best_model.predict(X_test))\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'{best_model_name} Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Plotting feature importance for the best model\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importances = best_model.feature_importances_\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x=feature_importances, y=range(len(feature_importances)))\n",
    "    plt.title(f'{best_model_name} Feature Importances')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.ylabel('Feature Index')\n",
    "    plt.show()\n",
    "\n",
    "# Plotting the training history of the neural network\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Neural Network Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Additional visualizations\n",
    "# Distribution of churn\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.countplot(x='Churn', data=df)\n",
    "plt.title('Distribution of Churn')\n",
    "plt.show()\n",
    "\n",
    "# Distribution of contract types\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.countplot(x='Contract', data=df)\n",
    "plt.title('Distribution of Contract Types')\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(df.corr(), annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
